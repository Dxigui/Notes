{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "决策树(decision tree)是一种基本的分类和回归方法.在分类问题中,决策树在对特征进行分类过程中可以被认为是一种 `if-then` 规则的集合.  \n",
    "决策树优点:  \n",
    "* 有良好的可读性\n",
    "* 分类速度快  \n",
    "\n",
    "决策树的学习包括:  \n",
    "* 特征选择\n",
    "* 决策树的生成\n",
    "* 决策树的修剪  \n",
    "\n",
    "决策树的算法:  \n",
    "* ID3 : 信息增益\n",
    "* C4.5 : 信息增益比\n",
    "* CART : gini 指数\n",
    "\n",
    "## 特征选择\n",
    "### 1. 信息增益选择特征\n",
    "选择合适的特征作为根节点能提升分类效果,特征的信息增益最大的为最优特征.  \n",
    "信息增益 = 经验熵 - 条件经验熵  \n",
    "$ g(D,A) = H(D) - H(D|A) $  \n",
    "* 经验熵: H(D) 为数据集 D 的经验熵  \n",
    "$ H(D) = -\\sum_{i=1}^{n}p_i{log_2{p_i}} $  \n",
    "$ p_i = P(X=x_i),i=1,2,...,n $  \n",
    "* 经验条件熵: H(Y|X) 特征 X 对数据集 D 的经验熵  \n",
    "$ H(Y|X) = \\sum_{i=1}^{n}p_iH(Y|X=x_i) $  \n",
    "\n",
    "示例:  \n",
    "训练数据集 D,|D| 表示其样本容量,即样本个数.设 K 个类 $ C_k $,k=1,2,...,K,$ |C_k| $ 为属于类 $ C_k $ 的样本个数,$ \\sum_{k=1}^{K}|C_k|=|D| $.社特征 A 有 n 个不同的取值 {$ a_1,a_2,...,a_n $},根据特征 A 的取值将 D 划分为 n 个子集 {$ D_1,D_2,...,D_3 $},$ |D_i| $ 为 $ D_i $ 的样本个数,$ \\sum_{i=1}^{n}|D_i|=|D| $,子集 $ D_i $ 中数据类 $ C_k $ 的样本的集合为 $ D_{ik} $,即 $ D_{ik}={D_i}\\bigcap{C_k},{D_{ik}} 为 D_{ik} $ 的样本个数.  \n",
    "1. 计算数据集 D 的经验熵 H(D)  \n",
    "$ H(D) = -\\sum_{k=1}^{k}\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|} $  \n",
    "2. 计算特征 A 对数据集 D 的经验条件熵 H(D|A)  \n",
    "$ H(D|A) = \\sum_{i=1}^{n}\\frac{|C_i|}{|D|}H(D_i) = -\\sum_{i=1}^{n}\\frac{|C_i|}{|D|}\\sum_{k=1}^{k}\\frac{|D_ik|}{|Di|}log_2\\frac{|D_ik|}{|Di|} $  \n",
    "3. 计算信息增益  \n",
    "$ g(D,A) = H(D) - H(D|A) $  \n",
    "\n",
    "### 2. 信息增益比选择特征  \n",
    "信息增益比是对信息增益的校正.当分类困难,训练数据集的经验熵过大,信息增益值会偏大,反之偏小,这时就需要对其进行校正.  \n",
    "* 信息增益比 = 信息增益 / 经验熵  \n",
    "$ g_R(D,A) = \\frac{g(D,A)}{H(D)} $  \n",
    "\n",
    "## 决策树的生成  \n",
    "### ID3 算法 \n",
    "ID3 算法是基于信息增益准则选择特征来构建决策树的各个结点,递归生成节点.  \n",
    "从根节点开始,对每个结点都计算特征的信息增益,选择信息增益最大的作为结点的特征,该特征的不同取值建立子结点;  \n",
    "构建决策时(数据集 D,特征集 A, 阈值 $ \\varepsilon $, 决策树 T):  \n",
    "1. 当 D 所有实例属于同一类 $ C_k $,则 T 为单结点树,并将类 $ C_k $ 作为该节点的类标记,返回 T;\n",
    "2. 若 A=$ \\varnothing $ ,则 T 为单结点树,并将 D 中实例数最大的类 $ C_k $ 作为该结点的类标记,返回 T;\n",
    "3. 否则,计算 A 中各个特征对 D 的信息增益,选择信息增益最大的特征 $ A_g $;\n",
    "4. 如果 $ A_g $ 的信息增益小于阈值 $ \\varepsilon $,则置 T 为大单结点树,并将 D中实例数最大的类 $ C_k $ 作为该结点的类标记,返回 T\n",
    "5. 否则,对 $ A_g $ 的每一个可能的值 $ a_i,依 A_g=a_i $ 将 D 分割为若干非空子集 $ D_i,将 D_i $ 中实例最大的类作为类标记,返回 T;\n",
    "6. 对第i个子结点,以 $ D_i $ 为训练集,以A-{$ A_g $}为特征集,递归地调用步(1)~步(5),得到子树 $ T_i $,返回 $ T_i $  \n",
    "\n",
    "### C4.5 算法\n",
    "C4.5 算法是基于信息增益比构建决策树\n",
    "## 决策树剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    datasets = [['青年', '否', '否', '一般', '否'],\n",
    "               ['青年', '否', '否', '好', '否'],\n",
    "               ['青年', '是', '否', '好', '是'],\n",
    "               ['青年', '是', '是', '一般', '是'],\n",
    "               ['青年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '好', '否'],\n",
    "               ['中年', '是', '是', '好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '好', '是'],\n",
    "               ['老年', '是', '否', '好', '是'],\n",
    "               ['老年', '是', '否', '非常好', '是'],\n",
    "               ['老年', '否', '否', '一般', '否'],\n",
    "               ]\n",
    "    columns = ['年龄', '有工作', '有自己的房子', '信贷情况', '类别']\n",
    "    # 返回数据集和每个维度的名称\n",
    "    return datasets, columns\n",
    "datasets, columns = create_data()\n",
    "train_data = pd.DataFrame(datasets, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empiricalEntropy(train_data, label):\n",
    "    \"\"\"\n",
    "    经验熵\n",
    "    train_data: 数据集\n",
    "    label: 目标集\n",
    "    \"\"\"\n",
    "    data_length = len(train_data)\n",
    "    labels_count = train_data[label].value_counts()\n",
    "    pi = [label / data_length for label in labels_count]\n",
    "    entropy = -sum([(p * log(p, 2)) for p in pi])\n",
    "    return entropy\n",
    "\n",
    "def empiricalConditionalEntropy(train_data, columns):\n",
    "    \"\"\"\n",
    "    条件经验熵\n",
    "    \"\"\"\n",
    "    data_length = len(train_data)\n",
    "    labels = pd.unique(train_data[columns[-1]])\n",
    "    labels_length = len(labels)\n",
    "    result = {}\n",
    "    for col in columns[:-1]:\n",
    "        attr_name = pd.unique(train_data[col])\n",
    "        attr_count = train_data[col].value_counts()\n",
    "        tmp = []\n",
    "        for name, count in zip(attr_name, attr_count):\n",
    "            # 特征的一个属性属于某类的概率\n",
    "            pi = [len(train_data[train_data[col] == name][train_data[columns[-1]] == label]) / count for label in labels]\n",
    "            entropy = -sum([p * log(p, 2) for p in pi if p != 0.0]) * (count / data_length)\n",
    "            tmp.append(entropy)\n",
    "        result[col] = sum(tmp)\n",
    "    max_entropy = sorted(result.items(), key=lambda x: x[1])[0]\n",
    "    return max_entropy\n",
    "\n",
    "def infoGain(emp_en, con_en):\n",
    "    return emp_en - con_en\n",
    "\n",
    "def main(train_data, label, columns):\n",
    "    columns = columns + [label]\n",
    "    emp_en = empiricalEntropy(train_data, label)\n",
    "    con_en = empiricalConditionalEntropy(train_data, columns)\n",
    "    result = infoGain(emp_en, con_en[1])\n",
    "    text = '特征({})的信息增益最大为{}，选择为根节点特征'.format(con_en[0], result)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxigui/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'特征(有自己的房子)的信息增益最大为0.4199730940219749，选择为根节点特征'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(train_data, '类别', ['年龄', '有工作', '有自己的房子', '信贷情况'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建决策树\n",
    "1. ID3 算法生成决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    def __init__(self, root=True, label=None, feature_name=None, feature=None):\n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.feature_name = feature_name\n",
    "        self.feature = feature\n",
    "        self.tree = {}\n",
    "        self.result = {'label:': self.label, 'feature:': self.feature,\n",
    "                       'tree:': self.tree}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}'.format(self.result)\n",
    "    \n",
    "    def add_node(self, val, node):\n",
    "        self.tree[val] = node\n",
    "    \n",
    "    def predict(self, feature):\n",
    "        if self.root is True:\n",
    "            return self.label\n",
    "        return self.tree[feature[self.feature]].predict[feature]\n",
    "\n",
    "class DTree(object):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self._tree = {}\n",
    "        \n",
    "    @staticmethod\n",
    "    def empiricalEntropy(train_data, label):\n",
    "        \"\"\"\n",
    "        经验熵\n",
    "        train_data: 数据集\n",
    "        label: 目标\n",
    "        \"\"\"\n",
    "        data_length = len(train_data)\n",
    "        labels_count = train_data[label].value_counts()\n",
    "        pi = [label / data_length for label in labels_count]\n",
    "        entropy = -sum([(p * log(p, 2)) for p in pi])\n",
    "        return entropy\n",
    "\n",
    "    def empiricalConditionalEntropy(self, train_data, columns):\n",
    "        \"\"\"\n",
    "        条件经验熵\n",
    "        \"\"\"\n",
    "        data_length = len(train_data)\n",
    "        labels = pd.unique(train_data[columns[-1]])\n",
    "        labels_length = len(labels)\n",
    "        result = {}\n",
    "        for col in columns[:-1]:\n",
    "            attr_name = pd.unique(train_data[col])\n",
    "            attr_count = train_data[col].value_counts()\n",
    "            # 特征 a 的条件经验熵\n",
    "            tmp = []\n",
    "            for name, count in zip(attr_name, attr_count):\n",
    "                # 特征的一个属性属于某类的概率\n",
    "                pi = [len(train_data[train_data[col] == name][train_data[columns[-1]] == label]) / count for label in labels]\n",
    "                entropy = -sum([p * log(p, 2) for p in pi if p != 0.0]) * (count / data_length)\n",
    "                tmp.append(entropy)\n",
    "            result[col] = sum(tmp)\n",
    "        max_entropy = sorted(result.items(), key=lambda x: x[1])[0]\n",
    "        return max_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def infoGain(emp_en, con_en):\n",
    "        \"\"\"\n",
    "        信息增益\n",
    "        \"\"\"\n",
    "        return emp_en - con_en\n",
    "    \n",
    "    def maxInfoGain(self, train_data, label):\n",
    "        columns = list(train_data.columns)\n",
    "        index = columns.index(label)\n",
    "        columns = columns + [columns.pop(index)]\n",
    "        emp_en = empiricalEntropy(train_data, label)\n",
    "        con_en = self.empiricalConditionalEntropy(train_data, columns)\n",
    "        result = infoGain(emp_en, con_en[1])\n",
    "        # best = '特征({})的信息增益最大为{}，选择为根节点特征'.format(con_en[0], result)\n",
    "        return con_en[0], result\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        input: 数据集 D, 特征集 A, 阈值 eta\n",
    "        output: 决策时 T\n",
    "        \"\"\"\n",
    "        # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "        _, y_train, features = train_data.iloc[:, :-1], train_data.iloc[:, -1], train_data.columns[:-1]\n",
    "        label = train_data.columns[-1]\n",
    "        \n",
    "        # 当 D 所有实例属于同一类  𝐶𝑘 ,则 T 为单结点树,并将类  𝐶𝑘  作为该节点的类标记,返回 T;\n",
    "        if len(y_train.value_counts) == 1:\n",
    "            return TreeNode(root=True, label=y_train.iloc[0])\n",
    "        \n",
    "        # 若 A= ∅  ,则 T 为单结点树,并将 D 中实例数最大的类  𝐶𝑘  作为该结点的类标记,返回 T;\n",
    "        if len(features) == 0:\n",
    "            return TreeNode(root=True, label=train_data.iloc[:,-1].value_counts().sort_values(ascending=False).index[0])\n",
    "        \n",
    "        #否则,计算 A 中各个特征对 D 的信息增益,选择信息增益最大的特征  𝐴𝑔 ;\n",
    "        max_feature, max_info_gain = self.maxInfoGain(train_data, label)\n",
    "        \n",
    "        # 如果  𝐴𝑔  的信息增益小于阈值  𝜀 ,则置 T 为大单结点树,并将 D中实例数最大的类  𝐶𝑘  作为该结点的类标记,返回 T\n",
    "        if max_info_gain < self.epsilon:\n",
    "            return TreeNode(root=True, label=y_train.value_counts().values_sort(asceding=False).index[0])\n",
    "        \n",
    "        # 否则,对  𝐴𝑔  的每一个可能的值  𝑎𝑖,依𝐴𝑔=𝑎𝑖  将 D 分割为若干非空子集  𝐷𝑖,将𝐷𝑖  中实例最大的类作为类标记,返回 T;\n",
    "        tree_node = TreeNode(root=False, feature_name=max_feature, feature=max_f)\n",
    "        data = train_data.drop(max_feature, axis=1)\n",
    "        # 对第i个子结点,以  𝐷𝑖  为训练集,以A-{ 𝐴𝑔 }为特征集,递归地调用步(1)~步(5),得到子树  𝑇𝑖 ,返回  𝑇𝑖 \n",
    "        # 递归生成树\n",
    "        sub_tree = self.train(data)\n",
    "        tree_node.add_node(sub_tree)\n",
    "        return tree_node\n",
    "    \n",
    "    def fit(self, train_data):\n",
    "        self._tree = self.train(train_data)\n",
    "        return self._tree\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self._tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    return data[:, :2], data[:, -1]\n",
    "\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pic = export_graphviz(clf, out_file=\"mytree.pdf\")\n",
    "with open('mytree.pdf') as f:\n",
    "    dot_graph = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(dot_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
