# 机器学习

## 回归线性模型

1. 损失函数



2. 通过梯度下降法找最小损失值(可微分) gradient descent

* 局部最优解(local minimal)

* 驻点(saddle point)

* 凸函数 

* 过拟合(overfitting):

* 德尔塔函数(冲激函数):一个为 1,其他为 0 

* 正则化(regularization)

* adgrad

* 更复杂的线性方程不一定有更优解,其原因来自 bias and variance

  怎样判断 bias and variance大小

  1. 如果模型不能拟合训练样本,bias 过大,出现 underfitting
  2. 如果模型能拟合训练样本,但是在测试样本中差距大,variance 过大,出现 overfitting

  对于 bias 

  1. 重新设定模型,添加跟多的特征,
  2. 设置更复杂的模型

  正则化可能影响 bias 

* 交叉验证

  将 training set 分成 training set and validation set, 先用 training set 训练 模型,用 validation set 验证误差,得出最好的,再用 training set 进行验证,再测试 testing set.当然也可以分更多份,然后求 avg

* 截图























### One-hot Encoding

`One-hot Encoding` 是使用 N 位状态寄存器来对 N 个状态进行编码,每个状态都有他独立的寄存器.且在任意时候,只有一位有效.

```python
encoder = preprocessing.OneHotEncoder()
encoder.fit([
    [0, 2, 1, 12],
    [1, 3, 5, 3],
    [2, 3, 2, 12],
    [1, 2, 4, 3]
])
encoded_vector = encoder.transform([[2,3,5,3]]).toarray()
print(encoded_vector)
# [[ 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]
```

对每个独立的特征下的特征值进行 `One-Hot` 编码,上面的例子里的每一列都是属于某特征下的特征值(第一列 `[0, 1, 2, 1]`),总共有三种特征值(`[0, 1, 2]`),所以经编码后变成 `[100, 010, 001]` ,所以在对 `[[2, 3, 5, 3]]` 进行编码时,`2` 编码后为 `001`, 后面同理.





## K 近邻算法

原理: 给定测试样本,基于某种距离度量找出训练集中与其最靠近的 k 个训练样本,然后基于这 k 个邻居的信息来进行预测.

K 近邻是一种"懒惰学习(lazy learning)": 在训练阶段仅仅把样本保存,训练时间为 0,待收到测试样本后在处理

"急切学习(eager learning)": 在驯良阶段就进行学习处理



## 决策树



## 随机森林



## 支持向量机 SVM





# Google ML

## 线性回归

### 模型方程

1. 简单模型

$$
y^{'} = b + {w_1}{x_1}
$$

* $$ y^{'} $$  :指的是预测标签

* $$ {b} $$ : 指的是偏差,有时候为 $$ {w_0} $$
* $$ {w_1} $$ :特征 1 的权重

* $$ {w_1} $$:特征 1 的权重

* $$ {x_1} $$ : 指的是特征

2. 多特征复杂模型

$$
y^{'} = b + {w_1}{x_1} + {w_2}{x_2} + {w_3}{x_3}
$$

## 训练与损失

**训练模型**是通过标签样本来学习所有权重和偏差的预测值.

**损失**是标签和预测值的差别.

### 常见的损失函数

1. 平方损失 ( {L_2}  损失) 损失函数

单个样本损失
$$
({y} - {y}^{'})^2
$$

2. 均方误差 (MSE)

均方误差是每个样本的平均平方损失.即所有样本的平方损失和的平均值
$$
MSE = \frac{1}{N} \sum_{(x,y) \epsilon {D}}({y} - {prediction({x})})^2
$$

* $$ ({x,y}) $$ : 样本
  * $$ {x} $$ : 习性进行预测时的特征集
  * $$ {y} $$ : 样本的标签
* $$ {prediction(x)} $$ : 权重和偏差与特征集 $$ {x} $$ 结合的函数
* $$ {D} $$ : 包含对个有标签的样本的数据集, 即 $$ ({x,y}) $$
* $$ {N} $$ : $$ {D} $$ 中的样本数量

3. 均方根误差(RMSE)

## 降低损失

### 迭代方法

![](/home/dxigui/git_repositories/notes/Notes/img/iteration_loss.svg)

上面的图为迭代方法的执行流程,将特征输入模型得到预测值,然后通过损失函数计算损失值,判断是否达到全局最优,没有则调整参数继续通过模型预测,通过这中方式不断迭代,直到获得最小损失.

### 梯度下降

**梯度下降**可应用在迭代方法中的图中的*计算参数更新*部分,以迭代的方式调整参数,逐渐找到最佳权重和偏差,将损失降到最低

权重 $$ {w_1} $$ 和损失的图形始终是凸型

![](/home/dxigui/git_repositories/notes/Notes/img/gs.svg)

**梯度下降法**会计算损失曲线的起点处的梯度(**梯度** 是偏导数的矢量)

梯度始终指向损失函数中增长最快的方向